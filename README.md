
# FPT-MVP: Future-Prediction Tokenizer vs VAE (BAIR)

Minimal PyTorch prototype to compare **VAE** vs **FPT (Future-Prediction Tokenizer)** for robot policy evaluation in world models.

## Acknowledgments

This project is based on the code from [world-model-eval](https://github.com/world-model-eval/world-model-eval) by Julian Quevedo, Percy Liang, and Sherry Yang. We thank the original authors for their excellent work on "Evaluating Robot Policies in a World Model".

Original paper: [Evaluating Robot Policies in a World Model](https://arxiv.org/abs/2506.00613)

## Project Purpose

This project aims to compare the effectiveness of two different tokenization approaches for robot policy evaluation:

- **VAE (Variational Autoencoder)**: Traditional reconstruction-based tokenization
- **FPT (Future-Prediction Tokenizer)**: Novel approach that focuses on future state prediction

The comparison will help determine which tokenization method is more suitable for evaluating robot policies in world model settings.

*Note: This project is currently under development.*
